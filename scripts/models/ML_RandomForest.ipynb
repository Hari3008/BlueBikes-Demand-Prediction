{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "BlueBikes Demand Forecasting using Random Forest Regressor\n",
        "Simplified and optimized version for debugging and fast execution\n",
        "\"\"\"\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import time\n",
        "import pickle\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "def load_and_check_data(filepath):\n",
        "    \"\"\"Load data and perform basic checks\"\"\"\n",
        "    print(\"=\"*60)\n",
        "    print(\"Loading and Checking Data\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    try:\n",
        "        df = pd.read_csv(filepath)\n",
        "        print(f\"✓ Data loaded successfully\")\n",
        "        print(f\"  Shape: {df.shape}\")\n",
        "        print(f\"  Memory usage: {df.memory_usage().sum() / 1024**2:.2f} MB\")\n",
        "\n",
        "        # Check for demand column\n",
        "        if 'demand' not in df.columns:\n",
        "            print(\"ERROR: 'demand' column not found!\")\n",
        "            print(f\"Available columns: {df.columns.tolist()}\")\n",
        "            return None\n",
        "\n",
        "        # Check for infinities and extreme values\n",
        "        print(f\"\\nTarget variable (demand) statistics:\")\n",
        "        print(f\"  Mean: {df['demand'].mean():.2f}\")\n",
        "        print(f\"  Std: {df['demand'].std():.2f}\")\n",
        "        print(f\"  Min: {df['demand'].min()}\")\n",
        "        print(f\"  Max: {df['demand'].max()}\")\n",
        "        print(f\"  Nulls: {df['demand'].isna().sum()}\")\n",
        "\n",
        "        # Check for infinite values\n",
        "        inf_cols = df.columns[df.isin([np.inf, -np.inf]).any()].tolist()\n",
        "        if inf_cols:\n",
        "            print(f\"\\nWARNING: Infinite values found in columns: {inf_cols}\")\n",
        "            # Replace infinites with NaN\n",
        "            df = df.replace([np.inf, -np.inf], np.nan)\n",
        "\n",
        "        return df\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR loading data: {e}\")\n",
        "        return None\n",
        "\n",
        "def prepare_features(df):\n",
        "    \"\"\"Prepare features with basic cleaning\"\"\"\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"Preparing Features\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # List all potential feature columns (excluding target and metadata)\n",
        "    exclude_cols = ['demand', 'timestamp', 'station_id', 'weather_category', 'station_type']\n",
        "    feature_cols = [col for col in df.columns if col not in exclude_cols]\n",
        "\n",
        "    print(f\"Found {len(feature_cols)} feature columns\")\n",
        "\n",
        "    # Select only numeric columns\n",
        "    numeric_cols = df[feature_cols].select_dtypes(include=[np.number]).columns.tolist()\n",
        "    print(f\"Using {len(numeric_cols)} numeric features\")\n",
        "\n",
        "    X = df[numeric_cols].copy()\n",
        "    y = df['demand'].copy()\n",
        "\n",
        "    # Handle missing values\n",
        "    if X.isna().any().any():\n",
        "        print(f\"Missing values found. Filling with median...\")\n",
        "        X = X.fillna(X.median())\n",
        "\n",
        "    # Check for remaining issues\n",
        "    if X.isna().any().any():\n",
        "        print(\"WARNING: Still have NaN values after filling\")\n",
        "        X = X.fillna(0)\n",
        "\n",
        "    print(f\"Final feature matrix shape: {X.shape}\")\n",
        "    print(f\"Final target shape: {y.shape}\")\n",
        "\n",
        "    return X, y, numeric_cols\n",
        "\n",
        "def train_simple_baseline(X_train, y_train, X_test, y_test):\n",
        "    \"\"\"Train a simple baseline Random Forest\"\"\"\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"Training Simple Baseline Random Forest\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Very simple model for testing\n",
        "    print(\"Using minimal parameters: n_estimators=10, max_depth=10\")\n",
        "\n",
        "    model = RandomForestRegressor(\n",
        "        n_estimators=100,  # Very few trees for speed\n",
        "        max_depth=10,     # Limited depth\n",
        "        min_samples_split=5,\n",
        "        random_state=42,\n",
        "        n_jobs=-1,\n",
        "        verbose=1  # Show progress\n",
        "    )\n",
        "\n",
        "    print(\"Starting training...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Train with timeout check\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    train_time = time.time() - start_time\n",
        "    print(f\"Training completed in {train_time:.2f} seconds\")\n",
        "\n",
        "    # Make predictions\n",
        "    print(\"Making predictions...\")\n",
        "    train_pred = model.predict(X_train)\n",
        "    test_pred = model.predict(X_test)\n",
        "\n",
        "    # Calculate metrics\n",
        "    train_metrics = {\n",
        "        'RMSE': np.sqrt(mean_squared_error(y_train, train_pred)),\n",
        "        'MAE': mean_absolute_error(y_train, train_pred),\n",
        "        'R2': r2_score(y_train, train_pred)\n",
        "    }\n",
        "\n",
        "    test_metrics = {\n",
        "        'RMSE': np.sqrt(mean_squared_error(y_test, test_pred)),\n",
        "        'MAE': mean_absolute_error(y_test, test_pred),\n",
        "        'R2': r2_score(y_test, test_pred)\n",
        "    }\n",
        "\n",
        "    print(f\"\\nTrain Metrics: RMSE={train_metrics['RMSE']:.2f}, MAE={train_metrics['MAE']:.2f}, R2={train_metrics['R2']:.3f}\")\n",
        "    print(f\"Test Metrics:  RMSE={test_metrics['RMSE']:.2f}, MAE={test_metrics['MAE']:.2f}, R2={test_metrics['R2']:.3f}\")\n",
        "\n",
        "    return model, train_metrics, test_metrics\n",
        "\n",
        "def train_optimized_model(X_train, y_train, X_test, y_test):\n",
        "    \"\"\"Train an optimized model with manual parameter selection\"\"\"\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"Training Optimized Random Forest\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Test a few parameter combinations manually\n",
        "    param_sets = [\n",
        "        {'n_estimators': 50, 'max_depth': 20, 'min_samples_split': 10},\n",
        "        {'n_estimators': 100, 'max_depth': 15, 'min_samples_split': 5},\n",
        "        {'n_estimators': 50, 'max_depth': None, 'min_samples_split': 10, 'max_features': 0.5}\n",
        "    ]\n",
        "\n",
        "    best_score = float('inf')\n",
        "    best_model = None\n",
        "    best_params = None\n",
        "\n",
        "    # Use a validation split from training data\n",
        "    X_tr, X_val, y_tr, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
        "\n",
        "    for i, params in enumerate(param_sets, 1):\n",
        "        print(f\"\\nTesting parameter set {i}/{len(param_sets)}: {params}\")\n",
        "\n",
        "        model = RandomForestRegressor(\n",
        "            **params,\n",
        "            random_state=42,\n",
        "            n_jobs=-1\n",
        "        )\n",
        "\n",
        "        start_time = time.time()\n",
        "        model.fit(X_tr, y_tr)\n",
        "        train_time = time.time() - start_time\n",
        "\n",
        "        val_pred = model.predict(X_val)\n",
        "        val_rmse = np.sqrt(mean_squared_error(y_val, val_pred))\n",
        "\n",
        "        print(f\"  Training time: {train_time:.2f}s\")\n",
        "        print(f\"  Validation RMSE: {val_rmse:.2f}\")\n",
        "\n",
        "        if val_rmse < best_score:\n",
        "            best_score = val_rmse\n",
        "            best_model = model\n",
        "            best_params = params\n",
        "\n",
        "    print(f\"\\nBest parameters: {best_params}\")\n",
        "    print(f\"Best validation RMSE: {best_score:.2f}\")\n",
        "\n",
        "    # Retrain best model on full training data\n",
        "    print(\"\\nRetraining best model on full training set...\")\n",
        "    best_model.fit(X_train, y_train)\n",
        "\n",
        "    # Calculate final metrics\n",
        "    train_pred = best_model.predict(X_train)\n",
        "    test_pred = best_model.predict(X_test)\n",
        "\n",
        "    train_metrics = {\n",
        "        'RMSE': np.sqrt(mean_squared_error(y_train, train_pred)),\n",
        "        'MAE': mean_absolute_error(y_train, train_pred),\n",
        "        'R2': r2_score(y_train, train_pred)\n",
        "    }\n",
        "\n",
        "    test_metrics = {\n",
        "        'RMSE': np.sqrt(mean_squared_error(y_test, test_pred)),\n",
        "        'MAE': mean_absolute_error(y_test, test_pred),\n",
        "        'R2': r2_score(y_test, test_pred)\n",
        "    }\n",
        "\n",
        "    print(f\"\\nFinal Train Metrics: RMSE={train_metrics['RMSE']:.2f}, MAE={train_metrics['MAE']:.2f}, R2={train_metrics['R2']:.3f}\")\n",
        "    print(f\"Final Test Metrics:  RMSE={test_metrics['RMSE']:.2f}, MAE={test_metrics['MAE']:.2f}, R2={test_metrics['R2']:.3f}\")\n",
        "\n",
        "    return best_model, train_metrics, test_metrics, best_params\n",
        "\n",
        "def analyze_feature_importance(model, feature_names, top_n=15):\n",
        "    \"\"\"Analyze feature importance\"\"\"\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(f\"Top {top_n} Feature Importances\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    importance_df = pd.DataFrame({\n",
        "        'feature': feature_names,\n",
        "        'importance': model.feature_importances_\n",
        "    }).sort_values('importance', ascending=False)\n",
        "\n",
        "    for idx, row in importance_df.head(top_n).iterrows():\n",
        "        print(f\"{row['feature']:40}: {row['importance']:.4f}\")\n",
        "\n",
        "    return importance_df\n",
        "\n",
        "def compare_models(baseline_metrics, optimized_metrics):\n",
        "    \"\"\"Compare model performances\"\"\"\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"Model Comparison\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    print(\"\\n{:<20} {:>15} {:>15} {:>15}\".format(\"Metric\", \"Baseline\", \"Optimized\", \"Improvement\"))\n",
        "    print(\"-\" * 70)\n",
        "\n",
        "    for metric in ['RMSE', 'MAE', 'R2']:\n",
        "        baseline_val = baseline_metrics[metric]\n",
        "        optimized_val = optimized_metrics[metric]\n",
        "\n",
        "        if metric == 'R2':\n",
        "            improvement = (optimized_val - baseline_val) * 100\n",
        "            symbol = \"+\" if improvement > 0 else \"\"\n",
        "        else:\n",
        "            improvement = ((baseline_val - optimized_val) / baseline_val) * 100\n",
        "            symbol = \"+\" if improvement > 0 else \"\"\n",
        "\n",
        "        print(\"{:<20} {:>15.3f} {:>15.3f} {:>14}{:.1f}%\".format(\n",
        "            f\"Test {metric}:\", baseline_val, optimized_val, symbol, improvement\n",
        "        ))\n",
        "\n",
        "def save_model(model, filename):\n",
        "    with open(filename, 'wb') as f:\n",
        "        pickle.dump(model, f)\n",
        "    print(f\"✓ Model saved to {filename}\")\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main execution function\"\"\"\n",
        "    print(\"=\"*60)\n",
        "    print(\"BlueBikes Random Forest - Simplified Fast Version\")\n",
        "    print(\"=\"*60)\n",
        "    print(\"This version is optimized for speed and debugging\\n\")\n",
        "\n",
        "    # Load and check data\n",
        "    df = load_and_check_data('bluebikes_ml_ready.csv')\n",
        "    if df is None:\n",
        "        return\n",
        "\n",
        "    # Use entire dataset\n",
        "    print(f\"\\nUsing entire dataset with {len(df)} rows...\")\n",
        "\n",
        "    # Prepare features\n",
        "    X, y, feature_names = prepare_features(df)\n",
        "\n",
        "    # Create train/test split (simple random split for speed)\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"Creating Train/Test Split\")\n",
        "    print(\"=\"*60)\n",
        "    print(\"Using 80/20 random split (not temporal) for speed\")\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.2, random_state=42\n",
        "    )\n",
        "\n",
        "    print(f\"Train size: {len(X_train)}\")\n",
        "    print(f\"Test size: {len(X_test)}\")\n",
        "\n",
        "    # Train simple baseline\n",
        "    baseline_model, baseline_train, baseline_test = train_simple_baseline(\n",
        "        X_train, y_train, X_test, y_test\n",
        "    )\n",
        "\n",
        "    # Train optimized model\n",
        "    optimized_model, optimized_train, optimized_test, best_params = train_optimized_model(\n",
        "        X_train, y_train, X_test, y_test\n",
        "    )\n",
        "\n",
        "    save_model(baseline_model, \"baseline_random_forest.pkl\")\n",
        "    save_model(optimized_model, \"optimized_random_forest.pkl\")\n",
        "\n",
        "    # Compare models\n",
        "    compare_models(baseline_test, optimized_test)\n",
        "\n",
        "    # Feature importance from optimized model\n",
        "    feature_importance = analyze_feature_importance(optimized_model, feature_names)\n",
        "\n",
        "    return baseline_model, optimized_model, feature_importance\n",
        "\n",
        "# Run the analysis\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        baseline, optimized, importance = main()\n",
        "    except Exception as e:\n",
        "        print(f\"\\nERROR: {e}\")\n",
        "        print(\"\\nTrying minimal debug version...\")\n",
        "\n",
        "        # Ultra-minimal test\n",
        "        print(\"\\nCreating synthetic data for testing...\")\n",
        "        X_test = np.random.randn(1000, 10)\n",
        "        y_test = np.random.randn(1000)\n",
        "\n",
        "        rf_test = RandomForestRegressor(n_estimators=5, max_depth=5, random_state=42)\n",
        "        rf_test.fit(X_test[:800], y_test[:800])\n",
        "        pred = rf_test.predict(X_test[800:])\n",
        "        print(f\"Synthetic data test RMSE: {np.sqrt(mean_squared_error(y_test[800:], pred)):.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AKTNAFxacmBx",
        "outputId": "a3c5559b-9cf7-401b-c1e2-823fe949a827"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "BlueBikes Random Forest - Simplified Fast Version\n",
            "============================================================\n",
            "This version is optimized for speed and debugging\n",
            "\n",
            "============================================================\n",
            "Loading and Checking Data\n",
            "============================================================\n",
            "✓ Data loaded successfully\n",
            "  Shape: (798639, 43)\n",
            "  Memory usage: 235.35 MB\n",
            "\n",
            "Target variable (demand) statistics:\n",
            "  Mean: 2.50\n",
            "  Std: 2.41\n",
            "  Min: 1\n",
            "  Max: 64\n",
            "  Nulls: 0\n",
            "\n",
            "Using entire dataset with 798639 rows...\n",
            "\n",
            "============================================================\n",
            "Preparing Features\n",
            "============================================================\n",
            "Found 38 feature columns\n",
            "Using 33 numeric features\n",
            "Final feature matrix shape: (798639, 33)\n",
            "Final target shape: (798639,)\n",
            "\n",
            "============================================================\n",
            "Creating Train/Test Split\n",
            "============================================================\n",
            "Using 80/20 random split (not temporal) for speed\n",
            "Train size: 638911\n",
            "Test size: 159728\n",
            "\n",
            "============================================================\n",
            "Training Simple Baseline Random Forest\n",
            "============================================================\n",
            "Using minimal parameters: n_estimators=10, max_depth=10\n",
            "Starting training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed:  3.6min\n",
            "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:  7.7min finished\n",
            "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training completed in 461.69 seconds\n",
            "Making predictions...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    2.0s\n",
            "[Parallel(n_jobs=2)]: Done 100 out of 100 | elapsed:    4.2s finished\n",
            "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.4s\n",
            "[Parallel(n_jobs=2)]: Done 100 out of 100 | elapsed:    0.9s finished\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Metrics: RMSE=1.28, MAE=0.77, R2=0.720\n",
            "Test Metrics:  RMSE=1.32, MAE=0.79, R2=0.702\n",
            "\n",
            "============================================================\n",
            "Training Optimized Random Forest\n",
            "============================================================\n",
            "\n",
            "Testing parameter set 1/3: {'n_estimators': 50, 'max_depth': 20, 'min_samples_split': 10}\n",
            "  Training time: 348.21s\n",
            "  Validation RMSE: 1.20\n",
            "\n",
            "Testing parameter set 2/3: {'n_estimators': 100, 'max_depth': 15, 'min_samples_split': 5}\n",
            "  Training time: 549.32s\n",
            "  Validation RMSE: 1.21\n",
            "\n",
            "Testing parameter set 3/3: {'n_estimators': 50, 'max_depth': None, 'min_samples_split': 10, 'max_features': 0.5}\n",
            "  Training time: 202.14s\n",
            "  Validation RMSE: 1.22\n",
            "\n",
            "Best parameters: {'n_estimators': 50, 'max_depth': 20, 'min_samples_split': 10}\n",
            "Best validation RMSE: 1.20\n",
            "\n",
            "Retraining best model on full training set...\n",
            "\n",
            "Final Train Metrics: RMSE=0.83, MAE=0.51, R2=0.881\n",
            "Final Test Metrics:  RMSE=1.19, MAE=0.68, R2=0.759\n",
            "✓ Model saved to baseline_random_forest.pkl\n",
            "✓ Model saved to optimized_random_forest.pkl\n",
            "\n",
            "============================================================\n",
            "Model Comparison\n",
            "============================================================\n",
            "\n",
            "Metric                      Baseline       Optimized     Improvement\n",
            "----------------------------------------------------------------------\n",
            "Test RMSE:                     1.318           1.186              +10.0%\n",
            "Test MAE:                      0.787           0.680              +13.6%\n",
            "Test R2:                       0.702           0.759              +5.7%\n",
            "\n",
            "============================================================\n",
            "Top 15 Feature Importances\n",
            "============================================================\n",
            "subscriber_ratio                        : 0.3939\n",
            "demand_t_minus_1                        : 0.2103\n",
            "day_of_week_average_4w                  : 0.0767\n",
            "return_trip_probability                 : 0.0587\n",
            "average_age_bracket                     : 0.0323\n",
            "rolling_mean_7d                         : 0.0289\n",
            "rolling_std_7d                          : 0.0276\n",
            "average_trip_duration                   : 0.0272\n",
            "hour_of_day_cos                         : 0.0213\n",
            "hour_of_day_sin                         : 0.0161\n",
            "trend_coefficient_7d                    : 0.0112\n",
            "month_to_date_average                   : 0.0112\n",
            "temperature                             : 0.0107\n",
            "station_longitude                       : 0.0103\n",
            "wind_speed_mph                          : 0.0097\n",
            "\n",
            "============================================================\n",
            "Analysis Complete!\n",
            "============================================================\n",
            "\n",
            "If this version also hangs, there's likely an issue with:\n",
            "1. The data file (corrupt, too large, or wrong format)\n",
            "2. Infinite/NaN values in features\n",
            "3. Memory constraints\n",
            "\n",
            "Try checking the data file manually or using a smaller subset.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LMZJl-1rhFqA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
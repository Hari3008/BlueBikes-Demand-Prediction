{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0HHxFVTigy7p",
        "outputId": "46c7eb83-c28b-471c-e357-ca7b1cdc3e6c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "BlueBikes Feature Engineering Pipeline\n",
            "============================================================\n",
            "\n",
            "[1/8] Loading raw data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-553710563.py:269: DtypeWarning: Columns (13) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  trips_df = pd.read_csv(trips_path)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   âœ“ Loaded 1,999,446 trip records\n",
            "   âœ“ Loaded 8,784 weather records\n",
            "\n",
            "[2/8] Extracting station metadata...\n",
            "   âœ“ Processed 379 stations\n",
            "   âœ“ Created 15 neighborhood clusters\n",
            "\n",
            "[3/8] Engineering user demographics...\n",
            "\n",
            "[4/8] Aggregating to station Ã— hour level...\n",
            "   âœ“ Created 798,639 station-hour records\n",
            "\n",
            "[5/8] Engineering temporal features...\n",
            "   âœ“ Added cyclical encodings and flags\n",
            "\n",
            "[6/8] Engineering spatial features...\n",
            "   âœ“ Added station metadata and clusters\n",
            "\n",
            "[7/8] Calculating historical demand features...\n",
            "   âœ“ Added lag features and rolling statistics\n",
            "\n",
            "[8/8] Merging weather data...\n",
            "   âœ“ Merged weather features\n",
            "\n",
            "============================================================\n",
            "PIPELINE COMPLETE\n",
            "============================================================\n",
            "\n",
            "ðŸ“Š Dataset Summary:\n",
            "   â€¢ Total records: 798,639\n",
            "   â€¢ Date range: 2020-01-01 00:00:00 to 2020-11-30 23:00:00\n",
            "   â€¢ Number of stations: 379\n",
            "   â€¢ Total features: 40  (excluding keys + target)\n",
            "\n",
            "ðŸ“‹ Feature Breakdown:\n",
            "   â€¢ Temporal: 13\n",
            "   â€¢ Spatial: 5\n",
            "   â€¢ Historical: 11\n",
            "   â€¢ Weather: 12\n",
            "   â€¢ User/Demographic: 4\n",
            "\n",
            "ðŸ’¾ Saving to: bluebikes_ml_ready.csv\n",
            "   âœ“ Saved successfully!\n",
            "\n",
            "âœ¨ Sample records:\n",
            "   station_id  timestamp  hour_of_day_sin  hour_of_day_cos  day_of_week_sin  day_of_week_cos  month_sin  month_cos  is_weekend  is_peak_hour  is_holiday  special_event_flag  station_latitude  station_longitude  station_capacity  neighborhood_cluster_id station_type  demand_t_minus_1  demand_t_minus_24  demand_t_minus_168  rolling_mean_7d  rolling_std_7d  same_hour_previous_week  month_to_date_average  day_of_week_average_4w  trend_coefficient_7d  temperature  feels_like_temperature  precipitation_mm  wind_speed_mph  weather_severity_score  subscriber_ratio  average_trip_duration  return_trip_probability  average_age_bracket  demand weather_category  weather_severity_score  weather_clear  weather_heavy_rain  weather_hot  weather_rain  weather_windy\n",
            "0           7 2020-01-01              0.0              1.0         0.974928        -0.222521        0.5   0.866025           0             0           1                   0         42.353391         -71.044571                16                       14     downtown               0.0                0.0                 0.0              0.0             0.0                      0.0                    0.0                     0.0                   0.0         34.6                56.88522               0.0             8.5                       2               1.0                  657.0                      0.0                  3.0       1              hot                       2          False               False         True         False          False\n",
            "1           8 2020-01-01              0.0              1.0         0.974928        -0.222521        0.5   0.866025           0             0           1                   0         42.353334         -71.137313                21                        0     suburban               0.0                0.0                 0.0              0.0             0.0                      0.0                    0.0                     0.0                   0.0         34.6                56.88522               0.0             8.5                       2               0.0                  420.5                      1.0                  3.0       2              hot                       2          False               False         True         False          False\n",
            "2          11 2020-01-01              0.0              1.0         0.974928        -0.222521        0.5   0.866025           0             0           1                   0         42.338629         -71.106500                31                       12     suburban               0.0                0.0                 0.0              0.0             0.0                      0.0                    0.0                     0.0                   0.0         34.6                56.88522               0.0             8.5                       2               1.0                  330.0                      0.0                  1.0       1              hot                       2          False               False         True         False          False\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "def create_cyclical_features(df, col, max_val, prefix=None):\n",
        "    \"\"\"Create sine and cosine encodings for cyclical features\"\"\"\n",
        "    name = prefix if prefix else col\n",
        "    df[f'{name}_sin'] = np.sin(2 * np.pi * df[col] / max_val)\n",
        "    df[f'{name}_cos'] = np.cos(2 * np.pi * df[col] / max_val)\n",
        "    return df\n",
        "\n",
        "def engineer_temporal_features(df):\n",
        "    \"\"\"Extract and encode temporal features matching specification\"\"\"\n",
        "    # Extract basic temporal components\n",
        "    df['hour_of_day'] = df['timestamp'].dt.hour\n",
        "    df['day_of_week'] = df['timestamp'].dt.dayofweek\n",
        "    df['month'] = df['timestamp'].dt.month\n",
        "\n",
        "    # Cyclical encodings\n",
        "    df = create_cyclical_features(df, 'hour_of_day', 24)\n",
        "    df = create_cyclical_features(df, 'day_of_week', 7)\n",
        "    df = create_cyclical_features(df, 'month', 12)\n",
        "\n",
        "    # Weekend flag\n",
        "    df['is_weekend'] = (df['day_of_week'] >= 5).astype(int)\n",
        "\n",
        "    # Peak hour indicators (morning: 7-9, evening: 16-19)\n",
        "    df['is_morning_peak'] = ((df['hour_of_day'] >= 7) & (df['hour_of_day'] <= 9)).astype(int)\n",
        "    df['is_evening_peak'] = ((df['hour_of_day'] >= 16) & (df['hour_of_day'] <= 19)).astype(int)\n",
        "    df['is_peak_hour'] = (df['is_morning_peak'] | df['is_evening_peak']).astype(int)\n",
        "\n",
        "    # Holiday flag (will be added later with actual dates)\n",
        "\n",
        "    return df\n",
        "\n",
        "def create_station_clusters(station_coords, n_clusters=10):\n",
        "    \"\"\"Create neighborhood clusters using K-means on station coordinates\"\"\"\n",
        "    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
        "    clusters = kmeans.fit_predict(station_coords)\n",
        "    return clusters\n",
        "\n",
        "def engineer_spatial_features(df, station_info):\n",
        "    \"\"\"Encode spatial features matching specification\"\"\"\n",
        "    # Merge station metadata\n",
        "    df = df.merge(station_info[['station_id', 'station_latitude', 'station_longitude',\n",
        "                                 'neighborhood_cluster_id', 'station_capacity', 'station_type']],\n",
        "                  on='station_id', how='left')\n",
        "\n",
        "    return df\n",
        "\n",
        "def calculate_historical_demand(df):\n",
        "    \"\"\"Create lag and rolling features for demand\"\"\"\n",
        "    # Sort by station and time\n",
        "    df = df.sort_values(['station_id', 'timestamp'])\n",
        "\n",
        "    # Lag features\n",
        "    df['demand_t_minus_1'] = df.groupby('station_id')['demand'].shift(1)\n",
        "    df['demand_t_minus_1'] = df['demand_t_minus_1'].fillna(0)\n",
        "    df['demand_t_minus_24'] = df.groupby('station_id')['demand'].shift(24)\n",
        "    df['demand_t_minus_24'] = df['demand_t_minus_24'].fillna(0)\n",
        "    df['demand_t_minus_168'] = df.groupby('station_id')['demand'].shift(168)\n",
        "    df['demand_t_minus_168'] = df['demand_t_minus_168'].fillna(0)\n",
        "\n",
        "    # Same hour previous week\n",
        "    df['same_hour_previous_week'] = df.groupby('station_id')['demand'].shift(168)\n",
        "\n",
        "    # Rolling statistics (7 days = 168 hours)\n",
        "    df['rolling_mean_7d'] = df.groupby('station_id')['demand'].transform(\n",
        "        lambda x: x.shift(1).rolling(window=168, min_periods=24).mean()\n",
        "    )\n",
        "    df['rolling_std_7d'] = df.groupby('station_id')['demand'].transform(\n",
        "        lambda x: x.shift(1).rolling(window=168, min_periods=24).std()\n",
        "    )\n",
        "\n",
        "    # Month-to-date average\n",
        "    df['month_year'] = df['timestamp'].dt.to_period('M')\n",
        "    df['month_to_date_average'] = df.groupby(['station_id', 'month_year'])['demand'].transform(\n",
        "        lambda x: x.expanding().mean().shift(1)\n",
        "    )\n",
        "\n",
        "    # Day of week average over last 4 weeks\n",
        "    df['day_of_week_average_4w'] = df.groupby(['station_id', 'day_of_week'])['demand'].transform(\n",
        "        lambda x: x.shift(1).rolling(window=4*24, min_periods=24).mean()\n",
        "    )\n",
        "\n",
        "    # Trend coefficient (linear regression slope over last 7 days)\n",
        "    # Using a simpler, more stable method to avoid MKL issues\n",
        "    def calculate_trend(series):\n",
        "        try:\n",
        "            if len(series) < 2:\n",
        "                return 0\n",
        "            # Remove NaN values\n",
        "            clean_series = series.dropna()\n",
        "            if len(clean_series) < 2:\n",
        "                return 0\n",
        "            # Check for constant series\n",
        "            if np.std(clean_series) < 1e-10:\n",
        "                return 0\n",
        "\n",
        "            # Manual calculation of slope (more stable than polyfit)\n",
        "            x = np.arange(len(clean_series))\n",
        "            y = clean_series.values\n",
        "            n = len(x)\n",
        "\n",
        "            # Calculate slope using simple linear regression formula\n",
        "            x_mean = np.mean(x)\n",
        "            y_mean = np.mean(y)\n",
        "            numerator = np.sum((x - x_mean) * (y - y_mean))\n",
        "            denominator = np.sum((x - x_mean) ** 2)\n",
        "\n",
        "            if denominator < 1e-10:\n",
        "                return 0\n",
        "\n",
        "            slope = numerator / denominator\n",
        "            return slope\n",
        "        except (ValueError, RuntimeError):\n",
        "            return 0\n",
        "\n",
        "    df['trend_coefficient_7d'] = df.groupby('station_id')['demand'].transform(\n",
        "        lambda x: x.shift(1).rolling(window=168, min_periods=24).apply(calculate_trend, raw=False)\n",
        "    )\n",
        "\n",
        "    return df\n",
        "\n",
        "def calculate_weather_features(weather_df):\n",
        "    \"\"\"Engineer weather features from raw weather data\"\"\"\n",
        "    # Calculate feels-like temperature (simplified wind chill/heat index)\n",
        "    def feels_like(temp, wind_speed, humidity):\n",
        "        # Simplified formula\n",
        "        if temp < 10:  # Wind chill for cold\n",
        "            return 13.12 + 0.6215*temp - 11.37*(wind_speed**0.16) + 0.3965*temp*(wind_speed**0.16)\n",
        "        elif temp > 27:  # Heat index for warm\n",
        "            return temp + 0.5555 * ((humidity/100) * 6.112 * np.exp(17.67*temp/(temp+243.5)) - 10)\n",
        "        else:\n",
        "            return temp\n",
        "\n",
        "    weather_df['feels_like_temperature'] = weather_df.apply(\n",
        "        lambda row: feels_like(row['temp'], row['wind_speed'], row['humidity']), axis=1\n",
        "    )\n",
        "\n",
        "    # Rename columns\n",
        "    weather_df = weather_df.rename(columns={\n",
        "        'temp': 'temperature',\n",
        "        'precip': 'precipitation_mm',\n",
        "        'wind_speed': 'wind_speed_mph'\n",
        "    })\n",
        "\n",
        "    # Weather category based on conditions\n",
        "    def categorize_weather(row):\n",
        "        if row['precipitation_mm'] > 5:\n",
        "            return 'heavy_rain'\n",
        "        elif row['precipitation_mm'] > 0.5:\n",
        "            return 'rain'\n",
        "        elif row['wind_speed_mph'] > 20:\n",
        "            return 'windy'\n",
        "        elif row['temperature'] < 0:\n",
        "            return 'freezing'\n",
        "        elif row['temperature'] > 30:\n",
        "            return 'hot'\n",
        "        else:\n",
        "            return 'clear'\n",
        "\n",
        "    weather_df['weather_category'] = weather_df.apply(categorize_weather, axis=1)\n",
        "\n",
        "    # Weather severity score (0-10 scale, higher = worse conditions)\n",
        "    def severity_score(row):\n",
        "        score = 0\n",
        "        # Temperature extremes\n",
        "        if row['temperature'] < -5:\n",
        "            score += 3\n",
        "        elif row['temperature'] < 5:\n",
        "            score += 2\n",
        "        elif row['temperature'] > 35:\n",
        "            score += 3\n",
        "        elif row['temperature'] > 30:\n",
        "            score += 2\n",
        "\n",
        "        # Precipitation\n",
        "        if row['precipitation_mm'] > 10:\n",
        "            score += 4\n",
        "        elif row['precipitation_mm'] > 2:\n",
        "            score += 2\n",
        "        elif row['precipitation_mm'] > 0:\n",
        "            score += 1\n",
        "\n",
        "        # Wind\n",
        "        if row['wind_speed_mph'] > 25:\n",
        "            score += 3\n",
        "        elif row['wind_speed_mph'] > 15:\n",
        "            score += 1\n",
        "\n",
        "        return min(score, 10)\n",
        "\n",
        "    weather_df['weather_severity_score'] = weather_df.apply(severity_score, axis=1)\n",
        "\n",
        "    return weather_df\n",
        "\n",
        "def calculate_user_demographics(trips_df):\n",
        "    \"\"\"Aggregate user demographics at station-hour level\"\"\"\n",
        "    # Subscriber ratio\n",
        "    trips_df['is_subscriber'] = (trips_df['usertype'] == 'Subscriber').astype(int)\n",
        "\n",
        "    # Calculate age with safeguards\n",
        "    trips_df['age'] = trips_df['year'] - trips_df['birth year']\n",
        "    trips_df['age'] = trips_df['age'].clip(lower=16, upper=80)\n",
        "\n",
        "    # Age bracket (1: <25, 2: 25-40, 3: 40-60, 4: 60+)\n",
        "    trips_df['age_bracket'] = pd.cut(trips_df['age'],\n",
        "                                      bins=[0, 25, 40, 60, 100],\n",
        "                                      labels=[1, 2, 3, 4])\n",
        "    trips_df['age_bracket'] = trips_df['age_bracket'].astype(float)\n",
        "\n",
        "    # Return trip probability (same start and end station)\n",
        "    trips_df['is_return_trip'] = (\n",
        "        trips_df['start station id'] == trips_df['end station id']\n",
        "    ).astype(int)\n",
        "\n",
        "    return trips_df\n",
        "\n",
        "def get_station_metadata(trips_df):\n",
        "    \"\"\"Extract station-level metadata from trips data\"\"\"\n",
        "    # Get unique stations with coordinates\n",
        "    stations = trips_df.groupby('start station id').agg({\n",
        "        'start station name': 'first',\n",
        "        'start station latitude': 'first',\n",
        "        'start station longitude': 'first'\n",
        "    }).reset_index()\n",
        "\n",
        "    stations.columns = ['station_id', 'station_name', 'station_latitude', 'station_longitude']\n",
        "\n",
        "    # Estimate station capacity based on trip volume\n",
        "    trip_counts = trips_df.groupby('start station id').size()\n",
        "    stations['station_capacity'] = trip_counts.values\n",
        "    # Normalize to reasonable capacity range (10-50 bikes)\n",
        "    stations['station_capacity'] = (\n",
        "        10 + 40 * (stations['station_capacity'] - stations['station_capacity'].min()) /\n",
        "        (stations['station_capacity'].max() - stations['station_capacity'].min())\n",
        "    ).round().astype(int)\n",
        "\n",
        "    # Create neighborhood clusters\n",
        "    coords = stations[['station_latitude', 'station_longitude']].values\n",
        "    stations['neighborhood_cluster_id'] = create_station_clusters(coords, n_clusters=15)\n",
        "\n",
        "    # Station type based on location characteristics\n",
        "    # Simple heuristic: downtown vs suburban based on distance from city center\n",
        "    boston_center_lat, boston_center_lon = 42.3601, -71.0589\n",
        "    stations['dist_from_center'] = np.sqrt(\n",
        "        (stations['station_latitude'] - boston_center_lat)**2 +\n",
        "        (stations['station_longitude'] - boston_center_lon)**2\n",
        "    )\n",
        "    stations['station_type'] = pd.cut(stations['dist_from_center'],\n",
        "                                      bins=[0, 0.02, 0.05, 1],\n",
        "                                      labels=['downtown', 'urban', 'suburban'])\n",
        "\n",
        "    return stations\n",
        "\n",
        "def process_bluebikes_data(trips_path, weather_path, output_path='bluebikes_ml_ready.csv'):\n",
        "    \"\"\"\n",
        "    Main pipeline to create ML-ready dataset matching exact specification\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"=\" * 60)\n",
        "    print(\"BlueBikes Feature Engineering Pipeline\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    print(\"\\n[1/8] Loading raw data...\")\n",
        "    trips_df = pd.read_csv(trips_path)\n",
        "    trips_df['starttime'] = pd.to_datetime(trips_df['starttime'])\n",
        "    trips_df['stoptime'] = pd.to_datetime(trips_df['stoptime'])\n",
        "    print(f\"   âœ“ Loaded {len(trips_df):,} trip records\")\n",
        "\n",
        "    weather_df = pd.read_csv(weather_path)\n",
        "    weather_df['datetime'] = pd.to_datetime(weather_df['datetime'])\n",
        "    print(f\"   âœ“ Loaded {len(weather_df):,} weather records\")\n",
        "\n",
        "    print(\"\\n[2/8] Extracting station metadata...\")\n",
        "    station_info = get_station_metadata(trips_df)\n",
        "    print(f\"   âœ“ Processed {len(station_info)} stations\")\n",
        "    print(f\"   âœ“ Created {station_info['neighborhood_cluster_id'].nunique()} neighborhood clusters\")\n",
        "\n",
        "    print(\"\\n[3/8] Engineering user demographics...\")\n",
        "    trips_df = calculate_user_demographics(trips_df)\n",
        "\n",
        "    # Round to hourly timestamp (using 'h' instead of deprecated 'H')\n",
        "    trips_df['timestamp'] = trips_df['starttime'].dt.floor('h')\n",
        "\n",
        "    print(\"\\n[4/8] Aggregating to station Ã— hour level...\")\n",
        "    # Aggregate trips\n",
        "    agg_dict = {\n",
        "        'tripduration': 'count',  # This is demand\n",
        "        'is_subscriber': 'mean',\n",
        "        'tripduration': ['count', 'mean'],\n",
        "        'is_return_trip': 'mean',\n",
        "        'age_bracket': 'mean'\n",
        "    }\n",
        "\n",
        "    station_hour = trips_df.groupby(['start station id', 'timestamp']).agg({\n",
        "        'tripduration': ['count', 'mean'],\n",
        "        'is_subscriber': 'mean',\n",
        "        'is_return_trip': 'mean',\n",
        "        'age_bracket': 'mean'\n",
        "    }).reset_index()\n",
        "\n",
        "    # Flatten column names\n",
        "    station_hour.columns = ['station_id', 'timestamp', 'demand',\n",
        "                            'average_trip_duration', 'subscriber_ratio',\n",
        "                            'return_trip_probability', 'average_age_bracket']\n",
        "\n",
        "    print(f\"   âœ“ Created {len(station_hour):,} station-hour records\")\n",
        "\n",
        "    print(\"\\n[5/8] Engineering temporal features...\")\n",
        "    station_hour = engineer_temporal_features(station_hour)\n",
        "\n",
        "    # Add holidays for 2020\n",
        "    us_holidays_2020 = pd.to_datetime([\n",
        "        '2020-01-01', '2020-01-20', '2020-02-17', '2020-05-25',\n",
        "        '2020-07-04', '2020-09-07', '2020-10-12', '2020-11-11',\n",
        "        '2020-11-26', '2020-12-25'\n",
        "    ])\n",
        "    station_hour['is_holiday'] = station_hour['timestamp'].dt.date.isin(\n",
        "        us_holidays_2020.date\n",
        "    ).astype(int)\n",
        "\n",
        "    # Special events flag (simplified: major sporting events, marathons)\n",
        "    special_events_2020 = pd.to_datetime([\n",
        "        '2020-02-02',  # Super Bowl\n",
        "        '2020-03-17',  # St. Patrick's Day\n",
        "        '2020-07-04',  # Independence Day\n",
        "    ])\n",
        "    station_hour['special_event_flag'] = station_hour['timestamp'].dt.date.isin(\n",
        "        special_events_2020.date\n",
        "    ).astype(int)\n",
        "\n",
        "    print(f\"   âœ“ Added cyclical encodings and flags\")\n",
        "\n",
        "    print(\"\\n[6/8] Engineering spatial features...\")\n",
        "    station_hour = engineer_spatial_features(station_hour, station_info)\n",
        "    print(f\"   âœ“ Added station metadata and clusters\")\n",
        "\n",
        "    print(\"\\n[7/8] Calculating historical demand features...\")\n",
        "    station_hour = calculate_historical_demand(station_hour)\n",
        "    print(f\"   âœ“ Added lag features and rolling statistics\")\n",
        "\n",
        "    print(\"\\n[8/8] Merging weather data...\")\n",
        "    weather_df = calculate_weather_features(weather_df)\n",
        "    station_hour = pd.merge(station_hour, weather_df,\n",
        "                            left_on='timestamp', right_on='datetime', how='left')\n",
        "\n",
        "    # Forward fill missing weather\n",
        "    weather_cols = ['temperature', 'feels_like_temperature', 'precipitation_mm',\n",
        "                    'wind_speed_mph', 'humidity', 'pressure',\n",
        "                    'weather_severity_score']\n",
        "    station_hour[weather_cols] = station_hour[weather_cols].ffill()\n",
        "\n",
        "    # One-hot encode weather category\n",
        "    weather_dummies = pd.get_dummies(station_hour['weather_category'],\n",
        "                                     prefix='weather')\n",
        "    station_hour = pd.concat([station_hour, weather_dummies], axis=1)\n",
        "\n",
        "    print(f\"   âœ“ Merged weather features\")\n",
        "\n",
        "    # Fill remaining NaN in historical features (early observations)\n",
        "    historical_cols = [col for col in station_hour.columns\n",
        "                      if any(x in col for x in ['lag', 'rolling', 'average', 'trend', 'same_hour'])]\n",
        "    station_hour[historical_cols] = station_hour[historical_cols].fillna(0)\n",
        "\n",
        "    # Select final columns in specified order\n",
        "    final_columns = [\n",
        "        # Primary keys\n",
        "        'station_id', 'timestamp',\n",
        "        # Temporal\n",
        "        'hour_of_day_sin', 'hour_of_day_cos',\n",
        "        'day_of_week_sin', 'day_of_week_cos',\n",
        "        'month_sin', 'month_cos',\n",
        "        'is_weekend', 'is_peak_hour', 'is_holiday', 'special_event_flag',\n",
        "        # Spatial\n",
        "        'station_latitude', 'station_longitude',\n",
        "        'station_capacity', 'neighborhood_cluster_id', 'station_type',\n",
        "        # Historical demand\n",
        "        'demand_t_minus_1', 'demand_t_minus_24', 'demand_t_minus_168',\n",
        "        'rolling_mean_7d', 'rolling_std_7d',\n",
        "        'same_hour_previous_week', 'month_to_date_average',\n",
        "        'day_of_week_average_4w', 'trend_coefficient_7d',\n",
        "        # Weather\n",
        "        'temperature', 'feels_like_temperature',\n",
        "        'precipitation_mm', 'wind_speed_mph',\n",
        "        'weather_severity_score',\n",
        "        # User/demographic\n",
        "        'subscriber_ratio', 'average_trip_duration',\n",
        "        'return_trip_probability', 'average_age_bracket',\n",
        "        # Target\n",
        "        'demand'\n",
        "    ]\n",
        "\n",
        "    # Add weather category dummies\n",
        "    weather_dummy_cols = [col for col in station_hour.columns if col.startswith('weather_')]\n",
        "    final_columns.extend(weather_dummy_cols)\n",
        "\n",
        "    # Filter to final columns\n",
        "    final_df = station_hour[final_columns].copy()\n",
        "\n",
        "    # Sort by timestamp and station\n",
        "    final_df = final_df.sort_values(['timestamp', 'station_id']).reset_index(drop=True)\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"PIPELINE COMPLETE\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"\\nðŸ“Š Dataset Summary:\")\n",
        "    print(f\"   â€¢ Total records: {len(final_df):,}\")\n",
        "    print(f\"   â€¢ Date range: {final_df['timestamp'].min()} to {final_df['timestamp'].max()}\")\n",
        "    print(f\"   â€¢ Number of stations: {final_df['station_id'].nunique()}\")\n",
        "    print(f\"   â€¢ Total features: {len(final_df.columns) - 3}  (excluding keys + target)\")\n",
        "\n",
        "    print(f\"\\nðŸ“‹ Feature Breakdown:\")\n",
        "    temporal_features = [c for c in final_df.columns if any(x in c for x in\n",
        "                        ['hour', 'day', 'month', 'weekend', 'peak', 'holiday', 'event'])]\n",
        "    print(f\"   â€¢ Temporal: {len(temporal_features)}\")\n",
        "\n",
        "    spatial_features = [c for c in final_df.columns if any(x in c for x in\n",
        "                       ['latitude', 'longitude', 'capacity', 'cluster', 'type'])]\n",
        "    print(f\"   â€¢ Spatial: {len(spatial_features)}\")\n",
        "\n",
        "    historical_features = [c for c in final_df.columns if any(x in c for x in\n",
        "                          ['minus', 'rolling', 'same_hour', 'average', 'trend'])]\n",
        "    print(f\"   â€¢ Historical: {len(historical_features)}\")\n",
        "\n",
        "    weather_features = [c for c in final_df.columns if any(x in c for x in\n",
        "                       ['temperature', 'precipitation', 'wind', 'weather', 'humidity', 'pressure'])]\n",
        "    print(f\"   â€¢ Weather: {len(weather_features)}\")\n",
        "\n",
        "    demographic_features = [c for c in final_df.columns if any(x in c for x in\n",
        "                           ['subscriber', 'trip_duration', 'return_trip', 'age_bracket'])]\n",
        "    print(f\"   â€¢ User/Demographic: {len(demographic_features)}\")\n",
        "\n",
        "    print(f\"\\nðŸ’¾ Saving to: {output_path}\")\n",
        "    final_df.to_csv(output_path, index=False)\n",
        "    print(f\"   âœ“ Saved successfully!\")\n",
        "\n",
        "    print(f\"\\nâœ¨ Sample records:\")\n",
        "    print(final_df.head(3).to_string())\n",
        "\n",
        "    return final_df\n",
        "\n",
        "# Usage\n",
        "if __name__ == \"__main__\":\n",
        "    df = process_bluebikes_data(\n",
        "        trips_path='bluebikes_tripdata_2020.csv',\n",
        "        weather_path='boston_weather_2020.csv',\n",
        "        output_path='bluebikes_ml_ready.csv'\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('bluebikes_ml_ready.csv')\n",
        "na_counts = df.isnull().sum()\n",
        "print(\"Number of NA values per column:\")\n",
        "print(na_counts)"
      ],
      "metadata": {
        "id": "mw2cHjaTmRDf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4261aac-6c02-4280-b204-71b7d0ae2fb4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of NA values per column:\n",
            "station_id                  0\n",
            "timestamp                   0\n",
            "hour_of_day_sin             0\n",
            "hour_of_day_cos             0\n",
            "day_of_week_sin             0\n",
            "day_of_week_cos             0\n",
            "month_sin                   0\n",
            "month_cos                   0\n",
            "is_weekend                  0\n",
            "is_peak_hour                0\n",
            "is_holiday                  0\n",
            "special_event_flag          0\n",
            "station_latitude            0\n",
            "station_longitude           0\n",
            "station_capacity            0\n",
            "neighborhood_cluster_id     0\n",
            "station_type                1\n",
            "demand_t_minus_1            0\n",
            "demand_t_minus_24           0\n",
            "demand_t_minus_168          0\n",
            "rolling_mean_7d             0\n",
            "rolling_std_7d              0\n",
            "same_hour_previous_week     0\n",
            "month_to_date_average       0\n",
            "day_of_week_average_4w      0\n",
            "trend_coefficient_7d        0\n",
            "temperature                 0\n",
            "feels_like_temperature      0\n",
            "precipitation_mm            0\n",
            "wind_speed_mph              0\n",
            "weather_severity_score      0\n",
            "subscriber_ratio            0\n",
            "average_trip_duration       0\n",
            "return_trip_probability     0\n",
            "average_age_bracket         0\n",
            "demand                      0\n",
            "weather_category            0\n",
            "weather_severity_score.1    0\n",
            "weather_clear               0\n",
            "weather_heavy_rain          0\n",
            "weather_hot                 0\n",
            "weather_rain                0\n",
            "weather_windy               0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "G0LkdbkTByDF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "BlueBikes Demand Forecasting using Random Forest Regressor\n",
        "Simplified and optimized version for debugging and fast execution\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import time\n",
        "import pickle\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "def load_and_check_data(filepath):\n",
        "    \"\"\"Load data and perform basic checks\"\"\"\n",
        "    print(\"=\"*60)\n",
        "    print(\"Loading and Checking Data\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    try:\n",
        "        df = pd.read_csv(filepath)\n",
        "        print(f\"âœ“ Data loaded successfully\")\n",
        "        print(f\"  Shape: {df.shape}\")\n",
        "        print(f\"  Memory usage: {df.memory_usage().sum() / 1024**2:.2f} MB\")\n",
        "\n",
        "        # Check for demand column\n",
        "        if 'demand' not in df.columns:\n",
        "            print(\"ERROR: 'demand' column not found!\")\n",
        "            print(f\"Available columns: {df.columns.tolist()}\")\n",
        "            return None\n",
        "\n",
        "        # Check for infinities and extreme values\n",
        "        print(f\"\\nTarget variable (demand) statistics:\")\n",
        "        print(f\"  Mean: {df['demand'].mean():.2f}\")\n",
        "        print(f\"  Std: {df['demand'].std():.2f}\")\n",
        "        print(f\"  Min: {df['demand'].min()}\")\n",
        "        print(f\"  Max: {df['demand'].max()}\")\n",
        "        print(f\"  Nulls: {df['demand'].isna().sum()}\")\n",
        "\n",
        "        # Check for infinite values\n",
        "        inf_cols = df.columns[df.isin([np.inf, -np.inf]).any()].tolist()\n",
        "        if inf_cols:\n",
        "            print(f\"\\nWARNING: Infinite values found in columns: {inf_cols}\")\n",
        "            # Replace infinites with NaN\n",
        "            df = df.replace([np.inf, -np.inf], np.nan)\n",
        "\n",
        "        return df\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR loading data: {e}\")\n",
        "        return None\n",
        "\n",
        "def prepare_features(df):\n",
        "    \"\"\"Prepare features with basic cleaning\"\"\"\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"Preparing Features\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # List all potential feature columns (excluding target and metadata)\n",
        "    exclude_cols = ['demand', 'timestamp', 'station_id', 'weather_category', 'station_type']\n",
        "    feature_cols = [col for col in df.columns if col not in exclude_cols]\n",
        "\n",
        "    print(f\"Found {len(feature_cols)} feature columns\")\n",
        "\n",
        "    # Select only numeric columns\n",
        "    numeric_cols = df[feature_cols].select_dtypes(include=[np.number]).columns.tolist()\n",
        "    print(f\"Using {len(numeric_cols)} numeric features\")\n",
        "\n",
        "    X = df[numeric_cols].copy()\n",
        "    y = df['demand'].copy()\n",
        "\n",
        "    # Handle missing values\n",
        "    if X.isna().any().any():\n",
        "        print(f\"Missing values found. Filling with median...\")\n",
        "        X = X.fillna(X.median())\n",
        "\n",
        "    # Check for remaining issues\n",
        "    if X.isna().any().any():\n",
        "        print(\"WARNING: Still have NaN values after filling\")\n",
        "        X = X.fillna(0)\n",
        "\n",
        "    print(f\"Final feature matrix shape: {X.shape}\")\n",
        "    print(f\"Final target shape: {y.shape}\")\n",
        "\n",
        "    return X, y, numeric_cols\n",
        "\n",
        "def train_simple_baseline(X_train, y_train, X_test, y_test):\n",
        "    \"\"\"Train a simple baseline Random Forest\"\"\"\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"Training Simple Baseline Random Forest\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Very simple model for testing\n",
        "    print(\"Using minimal parameters: n_estimators=10, max_depth=10\")\n",
        "\n",
        "    model = RandomForestRegressor(\n",
        "        n_estimators=100,  # Very few trees for speed\n",
        "        max_depth=10,     # Limited depth\n",
        "        min_samples_split=5,\n",
        "        random_state=42,\n",
        "        n_jobs=-1,\n",
        "        verbose=1  # Show progress\n",
        "    )\n",
        "\n",
        "    print(\"Starting training...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Train with timeout check\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    train_time = time.time() - start_time\n",
        "    print(f\"Training completed in {train_time:.2f} seconds\")\n",
        "\n",
        "    # Make predictions\n",
        "    print(\"Making predictions...\")\n",
        "    train_pred = model.predict(X_train)\n",
        "    test_pred = model.predict(X_test)\n",
        "\n",
        "    # Calculate metrics\n",
        "    train_metrics = {\n",
        "        'RMSE': np.sqrt(mean_squared_error(y_train, train_pred)),\n",
        "        'MAE': mean_absolute_error(y_train, train_pred),\n",
        "        'R2': r2_score(y_train, train_pred)\n",
        "    }\n",
        "\n",
        "    test_metrics = {\n",
        "        'RMSE': np.sqrt(mean_squared_error(y_test, test_pred)),\n",
        "        'MAE': mean_absolute_error(y_test, test_pred),\n",
        "        'R2': r2_score(y_test, test_pred)\n",
        "    }\n",
        "\n",
        "    print(f\"\\nTrain Metrics: RMSE={train_metrics['RMSE']:.2f}, MAE={train_metrics['MAE']:.2f}, R2={train_metrics['R2']:.3f}\")\n",
        "    print(f\"Test Metrics:  RMSE={test_metrics['RMSE']:.2f}, MAE={test_metrics['MAE']:.2f}, R2={test_metrics['R2']:.3f}\")\n",
        "\n",
        "    return model, train_metrics, test_metrics\n",
        "\n",
        "def train_optimized_model(X_train, y_train, X_test, y_test):\n",
        "    \"\"\"Train an optimized model with manual parameter selection\"\"\"\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"Training Optimized Random Forest\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Test a few parameter combinations manually\n",
        "    param_sets = [\n",
        "        {'n_estimators': 50, 'max_depth': 20, 'min_samples_split': 10},\n",
        "        {'n_estimators': 100, 'max_depth': 15, 'min_samples_split': 5},\n",
        "        {'n_estimators': 50, 'max_depth': None, 'min_samples_split': 10, 'max_features': 0.5}\n",
        "    ]\n",
        "\n",
        "    best_score = float('inf')\n",
        "    best_model = None\n",
        "    best_params = None\n",
        "\n",
        "    # Use a validation split from training data\n",
        "    X_tr, X_val, y_tr, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
        "\n",
        "    for i, params in enumerate(param_sets, 1):\n",
        "        print(f\"\\nTesting parameter set {i}/{len(param_sets)}: {params}\")\n",
        "\n",
        "        model = RandomForestRegressor(\n",
        "            **params,\n",
        "            random_state=42,\n",
        "            n_jobs=-1\n",
        "        )\n",
        "\n",
        "        start_time = time.time()\n",
        "        model.fit(X_tr, y_tr)\n",
        "        train_time = time.time() - start_time\n",
        "\n",
        "        val_pred = model.predict(X_val)\n",
        "        val_rmse = np.sqrt(mean_squared_error(y_val, val_pred))\n",
        "\n",
        "        print(f\"  Training time: {train_time:.2f}s\")\n",
        "        print(f\"  Validation RMSE: {val_rmse:.2f}\")\n",
        "\n",
        "        if val_rmse < best_score:\n",
        "            best_score = val_rmse\n",
        "            best_model = model\n",
        "            best_params = params\n",
        "\n",
        "    print(f\"\\nBest parameters: {best_params}\")\n",
        "    print(f\"Best validation RMSE: {best_score:.2f}\")\n",
        "\n",
        "    # Retrain best model on full training data\n",
        "    print(\"\\nRetraining best model on full training set...\")\n",
        "    best_model.fit(X_train, y_train)\n",
        "\n",
        "    # Calculate final metrics\n",
        "    train_pred = best_model.predict(X_train)\n",
        "    test_pred = best_model.predict(X_test)\n",
        "\n",
        "    train_metrics = {\n",
        "        'RMSE': np.sqrt(mean_squared_error(y_train, train_pred)),\n",
        "        'MAE': mean_absolute_error(y_train, train_pred),\n",
        "        'R2': r2_score(y_train, train_pred)\n",
        "    }\n",
        "\n",
        "    test_metrics = {\n",
        "        'RMSE': np.sqrt(mean_squared_error(y_test, test_pred)),\n",
        "        'MAE': mean_absolute_error(y_test, test_pred),\n",
        "        'R2': r2_score(y_test, test_pred)\n",
        "    }\n",
        "\n",
        "    print(f\"\\nFinal Train Metrics: RMSE={train_metrics['RMSE']:.2f}, MAE={train_metrics['MAE']:.2f}, R2={train_metrics['R2']:.3f}\")\n",
        "    print(f\"Final Test Metrics:  RMSE={test_metrics['RMSE']:.2f}, MAE={test_metrics['MAE']:.2f}, R2={test_metrics['R2']:.3f}\")\n",
        "\n",
        "    return best_model, train_metrics, test_metrics, best_params\n",
        "\n",
        "def analyze_feature_importance(model, feature_names, top_n=15):\n",
        "    \"\"\"Analyze feature importance\"\"\"\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(f\"Top {top_n} Feature Importances\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    importance_df = pd.DataFrame({\n",
        "        'feature': feature_names,\n",
        "        'importance': model.feature_importances_\n",
        "    }).sort_values('importance', ascending=False)\n",
        "\n",
        "    for idx, row in importance_df.head(top_n).iterrows():\n",
        "        print(f\"{row['feature']:40}: {row['importance']:.4f}\")\n",
        "\n",
        "    return importance_df\n",
        "\n",
        "def compare_models(baseline_metrics, optimized_metrics):\n",
        "    \"\"\"Compare model performances\"\"\"\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"Model Comparison\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    print(\"\\n{:<20} {:>15} {:>15} {:>15}\".format(\"Metric\", \"Baseline\", \"Optimized\", \"Improvement\"))\n",
        "    print(\"-\" * 70)\n",
        "\n",
        "    for metric in ['RMSE', 'MAE', 'R2']:\n",
        "        baseline_val = baseline_metrics[metric]\n",
        "        optimized_val = optimized_metrics[metric]\n",
        "\n",
        "        if metric == 'R2':\n",
        "            improvement = (optimized_val - baseline_val) * 100\n",
        "            symbol = \"+\" if improvement > 0 else \"\"\n",
        "        else:\n",
        "            improvement = ((baseline_val - optimized_val) / baseline_val) * 100\n",
        "            symbol = \"+\" if improvement > 0 else \"\"\n",
        "\n",
        "        print(\"{:<20} {:>15.3f} {:>15.3f} {:>14}{:.1f}%\".format(\n",
        "            f\"Test {metric}:\", baseline_val, optimized_val, symbol, improvement\n",
        "        ))\n",
        "\n",
        "def save_model(model, filename):\n",
        "    with open(filename, 'wb') as f:\n",
        "        pickle.dump(model, f)\n",
        "    print(f\"âœ“ Model saved to {filename}\")\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main execution function\"\"\"\n",
        "    print(\"=\"*60)\n",
        "    print(\"BlueBikes Random Forest - Simplified Fast Version\")\n",
        "    print(\"=\"*60)\n",
        "    print(\"This version is optimized for speed and debugging\\n\")\n",
        "\n",
        "    # Load and check data\n",
        "    df = load_and_check_data('bluebikes_ml_ready.csv')\n",
        "    if df is None:\n",
        "        return\n",
        "\n",
        "    # Use entire dataset\n",
        "    print(f\"\\nUsing entire dataset with {len(df)} rows...\")\n",
        "\n",
        "    # Prepare features\n",
        "    X, y, feature_names = prepare_features(df)\n",
        "\n",
        "    # Create train/test split (simple random split for speed)\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"Creating Train/Test Split\")\n",
        "    print(\"=\"*60)\n",
        "    print(\"Using 80/20 random split (not temporal) for speed\")\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.2, random_state=42\n",
        "    )\n",
        "\n",
        "    print(f\"Train size: {len(X_train)}\")\n",
        "    print(f\"Test size: {len(X_test)}\")\n",
        "\n",
        "    # Train simple baseline\n",
        "    baseline_model, baseline_train, baseline_test = train_simple_baseline(\n",
        "        X_train, y_train, X_test, y_test\n",
        "    )\n",
        "\n",
        "    # Train optimized model\n",
        "    optimized_model, optimized_train, optimized_test, best_params = train_optimized_model(\n",
        "        X_train, y_train, X_test, y_test\n",
        "    )\n",
        "\n",
        "    save_model(baseline_model, \"baseline_random_forest.pkl\")\n",
        "    save_model(optimized_model, \"optimized_random_forest.pkl\")\n",
        "\n",
        "    # Compare models\n",
        "    compare_models(baseline_test, optimized_test)\n",
        "\n",
        "    # Feature importance from optimized model\n",
        "    feature_importance = analyze_feature_importance(optimized_model, feature_names)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"Analysis Complete!\")\n",
        "    print(\"=\"*60)\n",
        "    print(\"\\nIf this version also hangs, there's likely an issue with:\")\n",
        "    print(\"1. The data file (corrupt, too large, or wrong format)\")\n",
        "    print(\"2. Infinite/NaN values in features\")\n",
        "    print(\"3. Memory constraints\")\n",
        "    print(\"\\nTry checking the data file manually or using a smaller subset.\")\n",
        "\n",
        "    return baseline_model, optimized_model, feature_importance\n",
        "\n",
        "# Run the analysis\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        baseline, optimized, importance = main()\n",
        "    except Exception as e:\n",
        "        print(f\"\\nERROR: {e}\")\n",
        "        print(\"\\nTrying minimal debug version...\")\n",
        "\n",
        "        # Ultra-minimal test\n",
        "        print(\"\\nCreating synthetic data for testing...\")\n",
        "        X_test = np.random.randn(1000, 10)\n",
        "        y_test = np.random.randn(1000)\n",
        "\n",
        "        rf_test = RandomForestRegressor(n_estimators=5, max_depth=5, random_state=42)\n",
        "        rf_test.fit(X_test[:800], y_test[:800])\n",
        "        pred = rf_test.predict(X_test[800:])\n",
        "        print(f\"Synthetic data test RMSE: {np.sqrt(mean_squared_error(y_test[800:], pred)):.2f}\")\n",
        "        print(\"\\nIf this worked, the issue is with your data file!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AKTNAFxacmBx",
        "outputId": "42b74cdf-f7ad-4be9-da31-f2cde471b944"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "BlueBikes Random Forest - Simplified Fast Version\n",
            "============================================================\n",
            "This version is optimized for speed and debugging\n",
            "\n",
            "============================================================\n",
            "Loading and Checking Data\n",
            "============================================================\n",
            "âœ“ Data loaded successfully\n",
            "  Shape: (798639, 43)\n",
            "  Memory usage: 235.35 MB\n",
            "\n",
            "Target variable (demand) statistics:\n",
            "  Mean: 2.50\n",
            "  Std: 2.41\n",
            "  Min: 1\n",
            "  Max: 64\n",
            "  Nulls: 0\n",
            "\n",
            "Using entire dataset with 798639 rows...\n",
            "\n",
            "============================================================\n",
            "Preparing Features\n",
            "============================================================\n",
            "Found 38 feature columns\n",
            "Using 33 numeric features\n",
            "Final feature matrix shape: (798639, 33)\n",
            "Final target shape: (798639,)\n",
            "\n",
            "============================================================\n",
            "Creating Train/Test Split\n",
            "============================================================\n",
            "Using 80/20 random split (not temporal) for speed\n",
            "Train size: 638911\n",
            "Test size: 159728\n",
            "\n",
            "============================================================\n",
            "Training Simple Baseline Random Forest\n",
            "============================================================\n",
            "Using minimal parameters: n_estimators=10, max_depth=10\n",
            "Starting training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed:  3.8min\n",
            "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:  8.2min finished\n",
            "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training completed in 492.56 seconds\n",
            "Making predictions...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    2.0s\n",
            "[Parallel(n_jobs=2)]: Done 100 out of 100 | elapsed:    4.0s finished\n",
            "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.4s\n",
            "[Parallel(n_jobs=2)]: Done 100 out of 100 | elapsed:    0.9s finished\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Metrics: RMSE=1.28, MAE=0.77, R2=0.720\n",
            "Test Metrics:  RMSE=1.32, MAE=0.79, R2=0.702\n",
            "\n",
            "============================================================\n",
            "Training Optimized Random Forest\n",
            "============================================================\n",
            "\n",
            "Testing parameter set 1/3: {'n_estimators': 50, 'max_depth': 20, 'min_samples_split': 10}\n",
            "  Training time: 348.31s\n",
            "  Validation RMSE: 1.20\n",
            "\n",
            "Testing parameter set 2/3: {'n_estimators': 100, 'max_depth': 15, 'min_samples_split': 5}\n",
            "  Training time: 539.16s\n",
            "  Validation RMSE: 1.21\n",
            "\n",
            "Testing parameter set 3/3: {'n_estimators': 50, 'max_depth': None, 'min_samples_split': 10, 'max_features': 0.5}\n",
            "  Training time: 203.15s\n",
            "  Validation RMSE: 1.22\n",
            "\n",
            "Best parameters: {'n_estimators': 50, 'max_depth': 20, 'min_samples_split': 10}\n",
            "Best validation RMSE: 1.20\n",
            "\n",
            "Retraining best model on full training set...\n",
            "\n",
            "Final Train Metrics: RMSE=0.83, MAE=0.51, R2=0.881\n",
            "Final Test Metrics:  RMSE=1.19, MAE=0.68, R2=0.759\n",
            "âœ“ Model saved to baseline_random_forest.pkl\n",
            "âœ“ Model saved to optimized_random_forest.pkl\n",
            "\n",
            "============================================================\n",
            "Model Comparison\n",
            "============================================================\n",
            "\n",
            "Metric                      Baseline       Optimized     Improvement\n",
            "----------------------------------------------------------------------\n",
            "Test RMSE:                     1.318           1.186              +10.0%\n",
            "Test MAE:                      0.787           0.680              +13.6%\n",
            "Test R2:                       0.702           0.759              +5.7%\n",
            "\n",
            "============================================================\n",
            "Top 15 Feature Importances\n",
            "============================================================\n",
            "subscriber_ratio                        : 0.3939\n",
            "demand_t_minus_1                        : 0.2103\n",
            "day_of_week_average_4w                  : 0.0767\n",
            "return_trip_probability                 : 0.0587\n",
            "average_age_bracket                     : 0.0323\n",
            "rolling_mean_7d                         : 0.0289\n",
            "rolling_std_7d                          : 0.0276\n",
            "average_trip_duration                   : 0.0272\n",
            "hour_of_day_cos                         : 0.0213\n",
            "hour_of_day_sin                         : 0.0161\n",
            "trend_coefficient_7d                    : 0.0112\n",
            "month_to_date_average                   : 0.0112\n",
            "temperature                             : 0.0107\n",
            "station_longitude                       : 0.0103\n",
            "wind_speed_mph                          : 0.0097\n",
            "\n",
            "============================================================\n",
            "Analysis Complete!\n",
            "============================================================\n",
            "\n",
            "If this version also hangs, there's likely an issue with:\n",
            "1. The data file (corrupt, too large, or wrong format)\n",
            "2. Infinite/NaN values in features\n",
            "3. Memory constraints\n",
            "\n",
            "Try checking the data file manually or using a smaller subset.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LMZJl-1rhFqA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}